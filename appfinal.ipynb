{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2yhAv3e_bI8"
      },
      "outputs": [],
      "source": [
        "!wget -q -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "with tarfile.open('aclImdb_v1.tar.gz','r') as tar:\n",
        "  tar.extractall()"
      ],
      "metadata": {
        "id": "6HWpCCXi_55f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyprind\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trCr8mzfABox",
        "outputId": "ae4c3393-8666-4ad6-d279-229581a6615c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyprind\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyprind\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "amvoZGnrAG51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basepath = 'aclImdb'\n",
        "\n",
        "labels = {'pos':1, 'neg':0}\n"
      ],
      "metadata": {
        "id": "qfjlCHYkAMKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pbar = pyprind.ProgBar(50000)\n",
        "df = pd.DataFrame()\n",
        "for s in ('test','train'):\n",
        "  for l in ('pos','neg'):\n",
        "    path = os.path.join(basepath, s, l)\n",
        "    for file in sorted(os.listdir(path)):\n",
        "      with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
        "        txt = infile.read()\n",
        "      df = pd.concat([df, pd.DataFrame([txt, labels[l]]).T], ignore_index=True, axis=0)\n",
        "      pbar.update()\n",
        "\n",
        "df.columns = ['review', 'sentiment']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svizE4jhAQdi",
        "outputId": "65222ae4-4ec5-4c4d-8a65-d31791f4f955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:01:09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "df = df.reindex(np.random.permutation(df.index))\n",
        "df.to_csv('movie_data.csv', index=False, encoding='utf-8')\n"
      ],
      "metadata": {
        "id": "CRpu10rIA4E0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
        "df.head(3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "collapsed": true,
        "id": "YwnBirxBA9Vx",
        "outputId": "4ba4ed02-5e4f-42c5-f516-139f28669786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment\n",
              "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
              "1  OK... so... I really like Kris Kristofferson a...          0\n",
              "2  ***SPOILER*** Do not read this, if you think a...          0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ca0f3f50-1d88-45b7-bcd5-640575535f3e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ca0f3f50-1d88-45b7-bcd5-640575535f3e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ca0f3f50-1d88-45b7-bcd5-640575535f3e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ca0f3f50-1d88-45b7-bcd5-640575535f3e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-06dec214-85f3-40aa-a981-ae47ffeba9ea\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-06dec214-85f3-40aa-a981-ae47ffeba9ea')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-06dec214-85f3-40aa-a981-ae47ffeba9ea button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"Every time I think about this film I feel physically ill. To read such a great book and later discover there's a film of it was a great feeling. Years later and imagine my joy at switching on the sci-fi channel and finding it starts in just 5mins!!! Up go the titles and then uggg. If just a couple of things had changed OK. Everything is changed. Numerous characters are removed entirely new rubbish ones are added. The main hero is shrunk and de-aged by about 30 years, and hilariously his girlfriend/wife is now his mother! Even the dog is reduced to sub-lassie capabilities. This is truly appalling cinema at its absolute worst. I would quite happily remove my own toenails with pliers rather than sit through another horrific viewing, and I urge anyone thinking of watching this - please don't. If you own a copy burn it now, right now and think how much better your life would have been had this celluloid insult never occurred.\",\n          \"As with all environmentally aware films from the 1970s SOYLENT GREEN has a rather cheesy view of what ecological meltdown is . Overpopulation means there`s too many people to feed ? I was under the impression that famines were caused by either war or failed economic policies . Stalin`s policy in the Soviet Union in the 1930s left millions dead because of famine and to this day the greatest man made tragedy was Mao`s rural policy in China which led over 30 million starvation deaths in the 1950s . And let`s not forget the great famines in the horn of Africa in the 1980s and 90s which were to do with conflicts not overpopulation . You might like to also consider that two of the most heavily populated areas on Earth , Hong Kong and Macau , have never suffered a famine in modern times . Likewise the expansion of shanty towns around cities as seen here isn`t strictly down to overpopulation - it`s down to economic factors where people flock to cities to find better paid work than in the countryside ( It`s a symptom of industrial progress - not of too many births ) so the image of the streets of New York city being too congested to walk through and of having people sleep in stairwells is somewhat laughable<br /><br />But don`t be fooled into thinking SOYLENT GREEN is a pile of corny tree hugging crap because I consider this to be the best ecological film of the<br /><br />70s . It plays on the contempary audience`s knowledge of the world where Sol and Thorn are beside themselves with joy at finding fruit , brandy and fresh meat . Thorn gasps in amazement at having ice in his whisky , puffs on a cigarette and delivers the classic line \\\" If I could afford it I`d smoke two , maybe three of these a day \\\" . But it`s the visage of the euthanasia chamber that`s memorable as Thorn gazes at the images of wild animals , flowers , running water and snow covered mountains , a world Thorn`s generation has never known . This is a very haunting scene which makes SOYLENT GREEN a very memorable film , combined with the fact it features the final screen appearance of Edward G Robinson as the wise old Jew Sol Roth\",\n          \"Yah. I know. It has the name \\\"Sinatra\\\" in the title, so how bad can it be? Well, it's bad, trust me! I rented this thinking it was some movie I missed in the theaters. It's not. It's some garbage \\\"movie\\\" made by the folks at Showtime (cable station). Geez, these cable stations make a few bucks they think they can make whatever garbage movies they want! It's not good. I am as big a Sinatra fan as any sane man, but this movie was just dumb. Boring. Dull. Unfunny. Uninteresting. The only redeeming quality is that (assuming they did stick to the facts) you do learn about what happened to the captors of Frank Jr. Otherwise it's just a stupid film.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vectorización**"
      ],
      "metadata": {
        "id": "rRzFckyhBtZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count = CountVectorizer()\n",
        "docs = np.array(['The sun is shining',\n",
        "                 'The weather is sweet',\n",
        "                 'The sun is shining, the weather is sweet',\n",
        "                 'and one and one is two'])\n",
        "\n",
        "bag = count.fit_transform(docs)"
      ],
      "metadata": {
        "id": "_y1fmxC_Bs8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(count.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abjGzmtmB217",
        "outputId": "b737c141-018f-4869-b25a-c2e39e80c8d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bag.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iIk7DNCB8Lx",
        "outputId": "7bb3846c-9afa-410b-8389-963fea11051b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 0 1 1 0 1 0 0]\n",
            " [0 1 0 0 0 1 1 0 1]\n",
            " [0 2 0 1 1 1 2 0 1]\n",
            " [2 1 2 0 0 0 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf = TfidfTransformer(use_idf=True)\n",
        "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hg4luSI4CEG5",
        "outputId": "5caedaf0-3240-4b2a-f3c9-34ff89cbaa51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.37632116 0.         0.56855566 0.56855566 0.\n",
            "  0.46029481 0.         0.        ]\n",
            " [0.         0.37632116 0.         0.         0.         0.56855566\n",
            "  0.46029481 0.         0.56855566]\n",
            " [0.         0.4574528  0.         0.3455657  0.3455657  0.3455657\n",
            "  0.55953044 0.         0.3455657 ]\n",
            " [0.65680405 0.1713738  0.65680405 0.         0.         0.\n",
            "  0.         0.32840203 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[0, 'review'][-50:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Pcxhm7i9COOh",
        "outputId": "62e6d902-ae8e-4a5e-db19-ffdeec63e4ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'is seven.<br /><br />Title (Brazil): Not Available'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Limpieza**"
      ],
      "metadata": {
        "id": "Nk6JPBHkCY46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def preprocessor(text):\n",
        "  text = re.sub('<[^>]*>','',text)\n",
        "  emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "  text = (re.sub('[\\W]+', ' ',text.lower())+ ' '.join(emoticons).replace('-',''))\n",
        "  return text"
      ],
      "metadata": {
        "id": "ExDPwmSYCdOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor(df.loc[0,'review'][-50:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YGZoB8K8DLY4",
        "outputId": "b56a920b-c42a-4570-9270-44b5a9118ff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'is seven title brazil not available'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['review'] = df['review'].apply(preprocessor)"
      ],
      "metadata": {
        "id": "qGlHj3MYDQSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **tokenización**"
      ],
      "metadata": {
        "id": "IjKu-I0KDXc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(text):\n",
        "  return text.split()\n",
        "\n",
        "tokenizer('runners like running and thus they run')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2sEO4M5DoF_",
        "outputId": "770a209e-af88-45f0-a5bb-20153e250b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "def tokenizer_porter(text):\n",
        "  return [porter.stem(word) for word in text.split()]\n",
        "\n",
        "tokenizer_porter('runners like running and thus they run')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6pIoNJVEIXG",
        "outputId": "31b51957-5954-4fc9-8e1f-f7b6b16ea8f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPdh0TTKGivR",
        "outputId": "3c1837bb-f036-4f05-9898-677b3f0a9b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop = stopwords.words('english')\n",
        "[w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:] if w not in stop]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2fVJRdYGmhN",
        "outputId": "9d11e5ec-12bc-4ed9-da55-fe95df4bedf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['runner', 'like', 'run', 'run', 'lot']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count = CountVectorizer(stop_words='english',\n",
        "                        max_df=.1,\n",
        "                        max_features=5000)\n",
        "X = count.fit_transform(df['review'].values)"
      ],
      "metadata": {
        "id": "jSaK716FG35W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "lda = LatentDirichletAllocation(n_components=10,\n",
        "                                random_state=123,\n",
        "                                learning_method='batch')\n",
        "X_topics = lda.fit_transform(X)"
      ],
      "metadata": {
        "id": "lQAHENPxHZ6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda.components_.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3C56aHKNc5F",
        "outputId": "934d45f7-6f50-4039-bda8-db8550f85d50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 5000)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "2LH2xo5oOm9D",
        "outputId": "111a467f-b671-4e78-8bfb-4c4b80f5ccfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sklearn.feature_extraction.text.CountVectorizer"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>sklearn.feature_extraction.text.CountVectorizer</b><br/>def __init__(*, input=&#x27;content&#x27;, encoding=&#x27;utf-8&#x27;, decode_error=&#x27;strict&#x27;, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=&#x27;(?u)\\\\b\\\\w\\\\w+\\\\b&#x27;, ngram_range=(1, 1), analyzer=&#x27;word&#x27;, max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=np.int64)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py</a>Convert a collection of text documents to a matrix of token counts.\n",
              "\n",
              "This implementation produces a sparse representation of the counts using\n",
              "scipy.sparse.csr_matrix.\n",
              "\n",
              "If you do not provide an a-priori dictionary and you do not use an analyzer\n",
              "that does some kind of feature selection then the number of features will\n",
              "be equal to the vocabulary size found by analyzing the data.\n",
              "\n",
              "Read more in the :ref:`User Guide &lt;text_feature_extraction&gt;`.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "input : {&#x27;filename&#x27;, &#x27;file&#x27;, &#x27;content&#x27;}, default=&#x27;content&#x27;\n",
              "    - If `&#x27;filename&#x27;`, the sequence passed as an argument to fit is\n",
              "      expected to be a list of filenames that need reading to fetch\n",
              "      the raw content to analyze.\n",
              "\n",
              "    - If `&#x27;file&#x27;`, the sequence items must have a &#x27;read&#x27; method (file-like\n",
              "      object) that is called to fetch the bytes in memory.\n",
              "\n",
              "    - If `&#x27;content&#x27;`, the input is expected to be a sequence of items that\n",
              "      can be of type string or byte.\n",
              "\n",
              "encoding : str, default=&#x27;utf-8&#x27;\n",
              "    If bytes or files are given to analyze, this encoding is used to\n",
              "    decode.\n",
              "\n",
              "decode_error : {&#x27;strict&#x27;, &#x27;ignore&#x27;, &#x27;replace&#x27;}, default=&#x27;strict&#x27;\n",
              "    Instruction on what to do if a byte sequence is given to analyze that\n",
              "    contains characters not of the given `encoding`. By default, it is\n",
              "    &#x27;strict&#x27;, meaning that a UnicodeDecodeError will be raised. Other\n",
              "    values are &#x27;ignore&#x27; and &#x27;replace&#x27;.\n",
              "\n",
              "strip_accents : {&#x27;ascii&#x27;, &#x27;unicode&#x27;} or callable, default=None\n",
              "    Remove accents and perform other character normalization\n",
              "    during the preprocessing step.\n",
              "    &#x27;ascii&#x27; is a fast method that only works on characters that have\n",
              "    a direct ASCII mapping.\n",
              "    &#x27;unicode&#x27; is a slightly slower method that works on any characters.\n",
              "    None (default) does nothing.\n",
              "\n",
              "    Both &#x27;ascii&#x27; and &#x27;unicode&#x27; use NFKD normalization from\n",
              "    :func:`unicodedata.normalize`.\n",
              "\n",
              "lowercase : bool, default=True\n",
              "    Convert all characters to lowercase before tokenizing.\n",
              "\n",
              "preprocessor : callable, default=None\n",
              "    Override the preprocessing (strip_accents and lowercase) stage while\n",
              "    preserving the tokenizing and n-grams generation steps.\n",
              "    Only applies if ``analyzer`` is not callable.\n",
              "\n",
              "tokenizer : callable, default=None\n",
              "    Override the string tokenization step while preserving the\n",
              "    preprocessing and n-grams generation steps.\n",
              "    Only applies if ``analyzer == &#x27;word&#x27;``.\n",
              "\n",
              "stop_words : {&#x27;english&#x27;}, list, default=None\n",
              "    If &#x27;english&#x27;, a built-in stop word list for English is used.\n",
              "    There are several known issues with &#x27;english&#x27; and you should\n",
              "    consider an alternative (see :ref:`stop_words`).\n",
              "\n",
              "    If a list, that list is assumed to contain stop words, all of which\n",
              "    will be removed from the resulting tokens.\n",
              "    Only applies if ``analyzer == &#x27;word&#x27;``.\n",
              "\n",
              "    If None, no stop words will be used. In this case, setting `max_df`\n",
              "    to a higher value, such as in the range (0.7, 1.0), can automatically detect\n",
              "    and filter stop words based on intra corpus document frequency of terms.\n",
              "\n",
              "token_pattern : str or None, default=r&quot;(?u)\\\\b\\\\w\\\\w+\\\\b&quot;\n",
              "    Regular expression denoting what constitutes a &quot;token&quot;, only used\n",
              "    if ``analyzer == &#x27;word&#x27;``. The default regexp select tokens of 2\n",
              "    or more alphanumeric characters (punctuation is completely ignored\n",
              "    and always treated as a token separator).\n",
              "\n",
              "    If there is a capturing group in token_pattern then the\n",
              "    captured group content, not the entire match, becomes the token.\n",
              "    At most one capturing group is permitted.\n",
              "\n",
              "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
              "    The lower and upper boundary of the range of n-values for different\n",
              "    word n-grams or char n-grams to be extracted. All values of n such\n",
              "    such that min_n &lt;= n &lt;= max_n will be used. For example an\n",
              "    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
              "    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
              "    Only applies if ``analyzer`` is not callable.\n",
              "\n",
              "analyzer : {&#x27;word&#x27;, &#x27;char&#x27;, &#x27;char_wb&#x27;} or callable, default=&#x27;word&#x27;\n",
              "    Whether the feature should be made of word n-gram or character\n",
              "    n-grams.\n",
              "    Option &#x27;char_wb&#x27; creates character n-grams only from text inside\n",
              "    word boundaries; n-grams at the edges of words are padded with space.\n",
              "\n",
              "    If a callable is passed it is used to extract the sequence of features\n",
              "    out of the raw, unprocessed input.\n",
              "\n",
              "    .. versionchanged:: 0.21\n",
              "\n",
              "    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
              "    first read from the file and then passed to the given callable\n",
              "    analyzer.\n",
              "\n",
              "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
              "    When building the vocabulary ignore terms that have a document\n",
              "    frequency strictly higher than the given threshold (corpus-specific\n",
              "    stop words).\n",
              "    If float, the parameter represents a proportion of documents, integer\n",
              "    absolute counts.\n",
              "    This parameter is ignored if vocabulary is not None.\n",
              "\n",
              "min_df : float in range [0.0, 1.0] or int, default=1\n",
              "    When building the vocabulary ignore terms that have a document\n",
              "    frequency strictly lower than the given threshold. This value is also\n",
              "    called cut-off in the literature.\n",
              "    If float, the parameter represents a proportion of documents, integer\n",
              "    absolute counts.\n",
              "    This parameter is ignored if vocabulary is not None.\n",
              "\n",
              "max_features : int, default=None\n",
              "    If not None, build a vocabulary that only consider the top\n",
              "    `max_features` ordered by term frequency across the corpus.\n",
              "    Otherwise, all features are used.\n",
              "\n",
              "    This parameter is ignored if vocabulary is not None.\n",
              "\n",
              "vocabulary : Mapping or iterable, default=None\n",
              "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
              "    indices in the feature matrix, or an iterable over terms. If not\n",
              "    given, a vocabulary is determined from the input documents. Indices\n",
              "    in the mapping should not be repeated and should not have any gap\n",
              "    between 0 and the largest index.\n",
              "\n",
              "binary : bool, default=False\n",
              "    If True, all non zero counts are set to 1. This is useful for discrete\n",
              "    probabilistic models that model binary events rather than integer\n",
              "    counts.\n",
              "\n",
              "dtype : dtype, default=np.int64\n",
              "    Type of the matrix returned by fit_transform() or transform().\n",
              "\n",
              "Attributes\n",
              "----------\n",
              "vocabulary_ : dict\n",
              "    A mapping of terms to feature indices.\n",
              "\n",
              "fixed_vocabulary_ : bool\n",
              "    True if a fixed vocabulary of term to indices mapping\n",
              "    is provided by the user.\n",
              "\n",
              "stop_words_ : set\n",
              "    Terms that were ignored because they either:\n",
              "\n",
              "      - occurred in too many documents (`max_df`)\n",
              "      - occurred in too few documents (`min_df`)\n",
              "      - were cut off by feature selection (`max_features`).\n",
              "\n",
              "    This is only available if no vocabulary was given.\n",
              "\n",
              "See Also\n",
              "--------\n",
              "HashingVectorizer : Convert a collection of text documents to a\n",
              "    matrix of token counts.\n",
              "\n",
              "TfidfVectorizer : Convert a collection of raw documents to a matrix\n",
              "    of TF-IDF features.\n",
              "\n",
              "Notes\n",
              "-----\n",
              "The ``stop_words_`` attribute can get large and increase the model size\n",
              "when pickling. This attribute is provided only for introspection and can\n",
              "be safely removed using delattr or set to None before pickling.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; from sklearn.feature_extraction.text import CountVectorizer\n",
              "&gt;&gt;&gt; corpus = [\n",
              "...     &#x27;This is the first document.&#x27;,\n",
              "...     &#x27;This document is the second document.&#x27;,\n",
              "...     &#x27;And this is the third one.&#x27;,\n",
              "...     &#x27;Is this the first document?&#x27;,\n",
              "... ]\n",
              "&gt;&gt;&gt; vectorizer = CountVectorizer()\n",
              "&gt;&gt;&gt; X = vectorizer.fit_transform(corpus)\n",
              "&gt;&gt;&gt; vectorizer.get_feature_names_out()\n",
              "array([&#x27;and&#x27;, &#x27;document&#x27;, &#x27;first&#x27;, &#x27;is&#x27;, &#x27;one&#x27;, &#x27;second&#x27;, &#x27;the&#x27;, &#x27;third&#x27;,\n",
              "       &#x27;this&#x27;], ...)\n",
              "&gt;&gt;&gt; print(X.toarray())\n",
              "[[0 1 1 1 0 0 1 0 1]\n",
              " [0 2 0 1 0 1 1 0 1]\n",
              " [1 0 0 1 1 0 1 1 1]\n",
              " [0 1 1 1 0 0 1 0 1]]\n",
              "&gt;&gt;&gt; vectorizer2 = CountVectorizer(analyzer=&#x27;word&#x27;, ngram_range=(2, 2))\n",
              "&gt;&gt;&gt; X2 = vectorizer2.fit_transform(corpus)\n",
              "&gt;&gt;&gt; vectorizer2.get_feature_names_out()\n",
              "array([&#x27;and this&#x27;, &#x27;document is&#x27;, &#x27;first document&#x27;, &#x27;is the&#x27;, &#x27;is this&#x27;,\n",
              "       &#x27;second document&#x27;, &#x27;the first&#x27;, &#x27;the second&#x27;, &#x27;the third&#x27;, &#x27;third one&#x27;,\n",
              "       &#x27;this document&#x27;, &#x27;this is&#x27;, &#x27;this the&#x27;], ...)\n",
              " &gt;&gt;&gt; print(X2.toarray())\n",
              " [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
              " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
              " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
              " [0 0 1 0 1 0 1 0 0 0 0 0 1]]</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 931);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_top_words = 10\n",
        "feature_names = count.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "  print(f'Topic: {topic_idx + 1}')\n",
        "  print(' '.join([feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WypERv2Oyfn",
        "outputId": "638f9258-3bc5-411c-a19d-201d396a61a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: 1\n",
            "horror original comedy black house version classic genre white night\n",
            "Topic: 2\n",
            "worst minutes guy script money boring stupid waste wasn comedy\n",
            "Topic: 3\n",
            "book dvd read version watched original video remember tv novel\n",
            "Topic: 4\n",
            "family performance father beautiful mother woman role lives true performances\n",
            "Topic: 5\n",
            "series episode tv kids comedy episodes shows family fun season\n",
            "Topic: 6\n",
            "murder police wife john plays crime woman role thriller town\n",
            "Topic: 7\n",
            "documentary camera effects audience sense human use style art shot\n",
            "Topic: 8\n",
            "music song songs musical role dance rock performance star dancing\n",
            "Topic: 9\n",
            "horror effects guy dead budget gore killer special blood looks\n",
            "Topic: 10\n",
            "action war game fight american japanese hero battle animation fighting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bad_movie = X_topics[:, 1].argsort()[::-1]\n",
        "for iter_idx, movie_idx in enumerate(bad_movie[:3]):\n",
        "  print(f'Bad movie {iter_idx+1}')\n",
        "  print(df['review'][movie_idx][:300],'...')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tulnnodPBXj",
        "outputId": "7e254665-ad3f-490a-f0d3-2ee7ee33c4eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bad movie 1\n",
            "i have to admit when i went to see this movie i didn t really have high expectations but even with my low expectations i was totally and utterly disappointed basically luke wilson is a hot shot who tends to go out with slightly crazy girlfriends there s slight mention of a girl stalking him but that ...\n",
            "Bad movie 2\n",
            "larry bishop directs writes and leads this soft core porn plot less biker movie about nothing to do with anything to call this one of the worst movies of 2008 is being kind to the garbage that i spent money on while in theaters its one of the worst movies i have ever seen i felt sorry for the girls  ...\n",
            "Bad movie 3\n",
            "okay so i found out about this movie and i watched the preview read almost all the reviews and was having a hard time debating whether i should watch it or not before i even watched the movie i was emotionally weird on it i was so unsure if i was going to watch this and be disturbed for like a long  ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df.loc[:35000, 'review'].values\n",
        "y_train = df.loc[:35000, 'sentiment'].values\n",
        "X_test = df.loc[35000:,'review'].values\n",
        "y_test = df.loc[35000:, 'sentiment'].values"
      ],
      "metadata": {
        "id": "Oo_6b2pgPEJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "m7KlyJi0PSaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(strip_accents=None,\n",
        "                        lowercase=False,\n",
        "                        preprocessor=None,\n",
        "                        stop_words=None,\n",
        "                        tokenizer=tokenizer,\n",
        "                        ngram_range=(1,1))\n",
        "\n",
        "lr_tfidf = Pipeline([('vect',tfidf),\n",
        "                     ('clf', LogisticRegression(random_state=1000,\n",
        "                                                solver='liblinear',\n",
        "                                                C=100000000000000,\n",
        "                                                penalty='l1'\n",
        "                                                ))])"
      ],
      "metadata": {
        "id": "Z4EkLdClPksl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_tfidf.fit(X_train,y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "PYtuDGW5R6Mi",
        "outputId": "785fcea7-4550-4f7c-e24b-a2c2bbacf5f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vect',\n",
              "                 TfidfVectorizer(lowercase=False,\n",
              "                                 tokenizer=<function tokenizer at 0x77fe75ddd3f0>)),\n",
              "                ('clf',\n",
              "                 LogisticRegression(C=100000000000000, penalty='l1',\n",
              "                                    random_state=1000, solver='liblinear'))])"
            ],
            "text/html": [
              "<style>#sk-container-id-65 {color: black;background-color: white;}#sk-container-id-65 pre{padding: 0;}#sk-container-id-65 div.sk-toggleable {background-color: white;}#sk-container-id-65 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-65 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-65 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-65 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-65 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-65 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-65 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-65 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-65 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-65 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-65 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-65 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-65 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-65 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-65 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-65 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-65 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-65 div.sk-item {position: relative;z-index: 1;}#sk-container-id-65 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-65 div.sk-item::before, #sk-container-id-65 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-65 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-65 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-65 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-65 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-65 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-65 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-65 div.sk-label-container {text-align: center;}#sk-container-id-65 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-65 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-65\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
              "                 TfidfVectorizer(lowercase=False,\n",
              "                                 tokenizer=&lt;function tokenizer at 0x77fe75ddd3f0&gt;)),\n",
              "                (&#x27;clf&#x27;,\n",
              "                 LogisticRegression(C=100000000000000, penalty=&#x27;l1&#x27;,\n",
              "                                    random_state=1000, solver=&#x27;liblinear&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-193\" type=\"checkbox\" ><label for=\"sk-estimator-id-193\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;,\n",
              "                 TfidfVectorizer(lowercase=False,\n",
              "                                 tokenizer=&lt;function tokenizer at 0x77fe75ddd3f0&gt;)),\n",
              "                (&#x27;clf&#x27;,\n",
              "                 LogisticRegression(C=100000000000000, penalty=&#x27;l1&#x27;,\n",
              "                                    random_state=1000, solver=&#x27;liblinear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-194\" type=\"checkbox\" ><label for=\"sk-estimator-id-194\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(lowercase=False,\n",
              "                tokenizer=&lt;function tokenizer at 0x77fe75ddd3f0&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-195\" type=\"checkbox\" ><label for=\"sk-estimator-id-195\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=100000000000000, penalty=&#x27;l1&#x27;, random_state=1000,\n",
              "                   solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_tfidf.score(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o68bXMr9SCHO",
        "outputId": "038f4e87-3854-41e7-d127-4ebd23d6faea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8316"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_tfidf.predict(np.array([\"This is awfull\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEn16-42ber2",
        "outputId": "baa3c3a9-a68f-47a7-b487-d036079e57ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(X_test[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Dt0zvXUbkXK",
        "outputId": "57808496-6b7f-48a8-f748-ee98bfb3637f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcyHk7O6br9m",
        "outputId": "dd71007b-02ba-424c-c094-2552f8dfdf65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 0, 1, 0, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "CgheRkiCbukg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"model.pickle\", \"wb\") as f:\n",
        "  pickle.dump(lr_tfidf, f)"
      ],
      "metadata": {
        "id": "l8d0Xl5kbxBa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}